<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">

<head>
	<!-- <meta name=viewport content="width=800"> -->
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	<script type="text/javascript" src="js/hidebib.js"></script>
	<style type="text/css">
		/* Color scheme stolen from Sergey Karayev */
		
		hr {
		border: 0;
		height: 0;
		border-top: 1px solid rgba(0, 0, 0, 0.1);
		border-bottom: 1px solid rgba(255, 255, 255, 0.3);
		}

		img {
		border-radius: 5%;
		}

		a {
		color: #1772d0;
		text-decoration: none;
		}
		
		a:focus,
		a:hover {
		color: #f09228;
		text-decoration: none;
		}
		
		body,
		td,
		th,
		tr,
		p,
		/* a {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px
		}
		
		strong {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		
		heading {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 30px;
		}
		
		papertitle {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		font-weight: 700;
		margin-bottom: 10cm;
		}
		
		name {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 32px;
		} */
		a {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		font-weight: 400
		}

		strong {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		font-weight: 600
		}

		heading {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 17px;
		font-weight: 600
		}

		papertitle {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		font-weight: 600
		}

		name {
		font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
		font-size: 32px;
		font-weight: 400
		}
		.zero {
		width: 160px;
		height: 80px;
		position: relative;
		}

		.one {
		width: 160px;
		height: 160px;
		position: relative;
		}
		
		.two {
		width: 160px;
		height: 160px;
		position: absolute;
		transition: opacity .2s ease-in-out;
		-moz-transition: opacity .2s ease-in-out;
		-webkit-transition: opacity .2s ease-in-out;
		}
		
		.fade {
		transition: opacity .2s ease-in-out;
		-moz-transition: opacity .2s ease-in-out;
		-webkit-transition: opacity .2s ease-in-out;
		}
		
		mid {
		font-size: 40px;
		position:relative;
		top:2px;
		}

		span.highlight {
		background-color: #ffffd0;
		}


	#summary:hover + #detail, #detail:hover {
	display: block;
	}
	#detail {
	display: none;
	}

	details summary > * {
		display: inline;
	}
	summary a * {
		pointer-events: none;
		} 
		details summary::-webkit-details-marker {
	display:none;
	}
	</style>
	
	<link rel="icon" href="misc/t_favicon.png">

	<title>Tanmay Shankar</title>
	<link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic" rel="stylesheet" type="text/css">
	<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  	
	<!-- TANMAY's TAGS -->
	<!-- Google Tag Manager -->
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-NLLBQLT');</script>
	<!-- End Google Tag Manager -->
	
</head>


<body>

	<!-- TANMAY's TAGS -->
	<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NLLBQLT"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<!-- End Google Tag Manager (noscript) -->

  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
	<tbody><tr>
	  <td>

		<!-- ###################################################################### -->
		<!-- Intro -->
		<!-- ###################################################################### -->

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td width="75%" valign="middle">
			  	<p align="center">
					<name>Tanmay Shankar</name>
					<!-- <name>Tanmay Shankar <mid><font color=#FF0000>|</font></mid> <font color=#C0C0C0>Suddhu</font></name> -->
			  	<p align="center">
					<font size="3"> tanmayshankar <font color=#FF0000>[at]</font>  cmu <font color=#FF0000>[dot]</font> edu </font>
			  	</p>
			  	<p>
					<p style = "text-align:justify">I'm a final-year PhD candidate in the <A href="http://www.ri.cmu.edu/" target="_blank">Robotics Institute</A> at <A href="http://www.cmu.edu" target="_blank"> Carnegie Mellon University</A>, where I work with <A href="https://www.cs.cmu.edu/~./jeanoh/" target="_blank">Jean Oh</a>. <b> <font color=#9D3894> I am on the job market for research and applied research roles in industry, starting in the fall of 2024. Please reach out if you'd like to chat! </font> </b> 
				</p>
				<p>	
					<p style = "text-align:justify">My Ph.D. research aims to learn and translate temporal abstractions (such as skills) across humans and robots. Here's a recording of my <a href="https://youtu.be/wbVPJU5p29w">recent proposal talk</a> for details, and here's a description of my <a href="javascript:toggleblock('researchphil')"> research interests and philosophy</a>! 
			  	</p>				
				  <div id="researchphil" class="researchphil">
					  <p>
							<p style = "text-align:justify"> 
							I'm interested in solving the human-to-robot imitation learning problem, particularly by building temporal abstractions of behavior across both humans and robots. Much of my work adopts a representation learning perspective to this problem, borrowing ideas from unsupervised learning, machine translation, and probalistic inference. I strongly believe in such interdisciplinary research; for example, my past work has made connections between cross-domain imtation learning and unsupervised machine translation, between value iteration and neural network architectural components, and more. 
							</p>
						  	<p style = "text-align:justify">
							I believe much of my research is not only applicable to robots, but also to dextrous prosthetic hands. I am passionate about how I can explore prosthetics as an application domain. This stems from a broader interest in the potential for assistive and rehabilitational robotics, an area I have been passionate about since my undergrad. 
					  		</p>
				  </div>
				  <script xml:space="preserve" language="JavaScript">
					  hideblock('researchphil');		
				  </script>

						
			  	<p>
					<p style = "text-align:justify">
			  		Before my Ph.D., I was a research engineer at Facebook AI Research (FAIR), Pittsburgh for 2 years, where I worked on unsupervised skill learning for robots with <A href="http://shubhtuls.github.io/" target="_blank">Shubham Tulsiani</a> and <A href="https://www.cs.cmu.edu/~./abhinavg/" target="_blank">Abhinav Gupta</a>. Before FAIR, I also did my Masters in Robotics from the Robotics Institute, working on differentiable imitation and reinforcement learning with <A href="https://kriskitani.github.io/" target="_blank">Kris Kitani</A> and <A href="https://www.linkedin.com/in/katharina-muelling-a6260780" target="_blank">Katharina Muelling</A>. Here's more of my <a href="javascript:toggleblock('academichistory')"> academic history</a>.
			  	</p> 
					<div id="academichistory" class="academichistory">
						<p>
							<p style = "text-align:justify"> 
							In addition to working full time at FAIR, I returned to FAIR Pittsburgh during a summer of my Ph.D., working with <a href="https://yixinlin.net/" target="_blank">Yixin Lin</a>,
							<a href="https://aravindr93.github.io/" target="_blank">Aravind Rajeswaran</a>,
							<a href="https://vikashplus.github.io/" target="_blank">Vikash Kumar</a>, and
							<a href="https://www.linkedin.com/in/stuartoanderson" target="_blank">Stuart Anderson</a> on translating skills across humans and robots. 							
							Before my MS, I did my undergrad at IIT Guwahati, where I worked on reinforcement learning networks, with Prithwijit Guha and S. K. Dwivedy. Before I worked on robot learning, I used to work on assistive technology - an area I'm also passionate about. During my undergrad, I also spent summers working with <A href="https://mykel.kochenderfer.com/" target="_blank">Mykel Kochenderfer</A> at the 
							<A href="https://sisl.stanford.edu/" target="_blank">Stanford Intelligent Systems Lab</A>, and with <A href="https://www.cs.cmu.edu/~./choset/" target="_blank">Howie Choset</A> at the <A href="http://biorobotics.ri.cmu.edu/" target="_blank">Biorobotics Lab</A> at CMU.  
						  </p>
					
					</div>
					<script xml:space="preserve" language="JavaScript">
						hideblock('academichistory');		
					</script>				

			  	
			  <p align=center>
				<strong>
				<a href="misc/CV/TanmayShankar_CV_Jan2024.pdf" target="_blank">CV</a> &nbsp/&nbsp
				<a href="https://scholar.google.com/citations?user=0k1qcvgAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
				<a href="https://github.com/tanmayshankar/" target="_blank"> Github </a> &nbsp/&nbsp
				<a href="https://www.linkedin.com/in/tanmayshankar1/" target="_blank"> LinkedIn </a>
				</strong>
			  </p>
			</td>
			<td width="50%">
			  <img src="misc/tanmay-portrait-zoom.jpg" style="width: 200; height: auto; border-radius: 5%;">
			</td>
		  </tr>
		</table>

		<hr>

		<!-- ###################################################################### -->
		<!-- Updates -->
		<!-- ###################################################################### -->

			<h2>Updates</h2>
		<table width="100%" align="center" border="0" cellspacing="6" cellpadding="0">
			<colgroup>
				<col span="1" style="width: 12%;">
				<col span="1" style="width: 88%;">
			</colgroup>
			<tbody>
			<tr>
				<td><p style="color:FF0000; display:inline;">[Mar '24] &nbsp</p></td>
				<td>Check out our real robot results on translating agent-environment interactions <a href="https://youtu.be/amEA4JuZxzg"> here! </a></td>
			</tr>				
			<tr>
				<td><p style="color:FF0000; display:inline;">[Mar '24] &nbsp</p></td>
				<td>Submitted our work on translating agent-environment interaction abstractions to IROS 2024!</td>
			</tr>	
			<tr>
				<td><p style="color:FF0000; display:inline;">[Oct '23] &nbsp</p></td>
				<td>I'm collaborating with the New Dexterity group from the University of Auckland on a new project!</td>
			</tr>
			<tr>
				<td><p style="color:FF0000; display:inline;">[Dec '22] &nbsp</p></td>
				<td>Presented my work on learning agent-environment interaction abstractions at the workshop on aligning human-robot representations at CoRL 2022.</td>
			</tr>
			<tr>
				<td><p style="color:FF0000; display:inline;">[Nov '22] &nbsp</p></td>
				<td>Successfully passed my Ph.D. thesis proposal! Here's a <a href="https://youtu.be/wbVPJU5p29w">recording of my talk!</a> </td> 
			</tr>
			  <!-- <tr>
				<td><p style="color:FF0000; display:inline">[Sep '22] &nbsp</p></td>
				<td><a href="https://suddhu.github.io/midastouch-tactile/">MidasTouch</a> was accepted to <a href="https://corl2022.org/">CoRL 2022</a> as an oral. 
				</td>
			  </tr> -->
		  </tbody>
		</tbody></table>
		</div>
		<br>
		<hr>
		
		<div style="height:20px;font-size:1px;">&nbsp;</div>

		<!-- ###################################################################### -->
		<!-- Research -->
		<!-- ###################################################################### -->
		
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
		  <tr>
			<td width="100%" valign="middle">
			  <h2>Robot Learning Research</h2>
			</td>
		  </tr>
		</table>    

	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">


		<!-- ###################################################################### -->
		<!-- H2R -->
		<!-- ###################################################################### -->

		
		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<!-- <img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/Images/TEMGS.JPG?raw=true' width="300"> -->
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/H2R2.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
					Translating Agent-Environment Interactions across Humans and Robots
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,
					<a href="https://www.linkedin.com/in/chaitanya1chawla/?originalSubdomain=de" target="_blank">C. Chawla</a>,
					<a href="https://www.linkedin.com/in/almutwakel-hassan-147772214/" target="_blank">A. Hassan</a>,
					<a href="https://www.cs.cmu.edu/~./jeanoh/" target="_blank">J. Oh</a>
					
						<div style="height:5px;font-size:1px;">&nbsp;</div>
						<font color=#696969> Submitted to Intelligent Conference on Intelligent Robots and Systems, IROS 2024 </font>													 
						<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
						[<font color=#009933>Oral: 6% acceptance rate</font>] -->
						<div style="height:5px;font-size:1px;">&nbsp;</div> 
					<!-- <a href="https://github.com/tanmayshankar/CausalSkillLearning" target="_blank">Code</a>					 -->					
					<a href="https://sites.google.com/view/interaction-abstractions" target="_blank">Website</a> /
					<a href="https://github.com/tanmayshankar/CausalSkillLearning" target="_blank">Code</a> /
					<a href="https://youtu.be/amEA4JuZxzg" target="_blank">Video</a>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">					
					My previous translation work (ICML 2022) focused on translating skills from humans to robots when they were executed in similar sequential contexts. Inspired by humans' ability to come up 
					with functionally equivalent skills that may be executed in entirely different contexts, I am now exploring whether we can translate skills across humans and robots when their effects on the environment are the same. 
					</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- TEMGS -->
		<!-- ###################################################################### -->

		
		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<!-- <img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/Images/TEMGS.JPG?raw=true' width="300"> -->
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/TEG.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
					<!-- Translating Robot Skills: Learning Unsupervised Skill Correspondences across Humans and Robots -->
					<!-- Learning Abstract Representations of Agent Environment Interactions -->
					Translating EMG Control Signals to Dextrous Robot and Prosthetic Hands
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,
					<a href="https://www.linkedin.com/in/bonnie-guan-4a922118b/?originalSubdomain=nz" target="_blank">B. Guan</a>, 
					<a href="https://www.linkedin.com/in/ricardo-vilela-de-godoy/?originalSubdomain=br" target="_blank">R. Vilela</a>, 
					<a href="https://minasliarokapis.com/" target="_blank">M. Liarkopis</a>, 
					<a href="https://www.cs.cmu.edu/~./jeanoh/" target="_blank">J. Oh</a>
					
						<div style="height:5px;font-size:1px;">&nbsp;</div>
						<font color=#696969> In Preparation</font>													 
						<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
						[<font color=#009933>Oral: 6% acceptance rate</font>] -->
						<div style="height:5px;font-size:1px;">&nbsp;</div> 
					<!-- <a href="https://github.com/tanmayshankar/CausalSkillLearning" target="_blank">Code</a>					 -->
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id=	"detail">
				<i>
					<p><p style = "text-align:justify">					
					Inspired by the success of my work translating skills across human and robot arms, along with collaborators from the University of Auckland, I'm exploring whether we can apply equivalent strategies to translating EMG signals to control dextrous robot and prosthetic hands. 
					<!-- We developed an unsupervised approach to learn correspondences between skills across humans and various morphologically different robots, taking inspiration from unsupervised machine translation. Our approach is able to learn semantically meaningful  orrespondences between skills across multiple robot-robot and human-robot domain pairs, despite being completely unsupervised. -->

					</p>
				</div>
			</td>
		</tr> 

		<!-- ###################################################################### -->
		<!-- LPS -->
		<!-- ###################################################################### -->
		
		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<!-- <img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/Images/TEMGS.JPG?raw=true' width="300"> -->
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/Images/LPS.jpg?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
					<!-- Translating Robot Skills: Learning Unsupervised Skill Correspondences across Humans and Robots -->
					<!-- Learning Abstract Representations of Agent Environment Interactions -->
					Learning New Painting Skills by Exploring Paint Stroke Representations
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>					
					<a href="https://lawrencedchen.com/" target="_blank">L. Chen</a>,
					<a href="https://pschaldenbrand.github.io/" target="_blank">P. Schaldenbrand</a>,
					<u>T. Shankar</u>,
					<a href="https://www.cs.cmu.edu/~./jeanoh/" target="_blank">J. Oh</a>
					
						<div style="height:5px;font-size:1px;">&nbsp;</div>
						<font color=#696969> In Preparation</font>													 
						<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
						[<font color=#009933>Oral: 6% acceptance rate</font>] -->
						<div style="height:5px;font-size:1px;">&nbsp;</div> 
					<!-- <a href="https://github.com/tanmayshankar/CausalSkillLearning" target="_blank">Code</a>					 -->
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">					
					Inspired by my previous work on representation learning for skill learning, and Peter Schaldenbrand's prior work on <a href="https://pschaldenbrand.github.io/frida/" target="_blank"> FRIDA the robot painter</a>, together with Lawrence Chen, we are exploring whether building learnt representations of paint strokes would facilitate learning new types of paint strokes beyond ones the robot is preprogrammed with, to improve FRIDA's artistic expression. 
					</p>
				</div>
			</td>
		</tr> 



		<!-- ###################################################################### -->
		<!-- LIA -->
		<!-- ###################################################################### -->
		
		<tr>
			<td width="25%" align="center">
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/LIA3.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
					<!-- Translating Robot Skills: Learning Unsupervised Skill Correspondences across Humans and Robots -->
					Learning Abstract Representations of Agent Environment Interactions
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,
					<!-- <a href="https://www.linkedin.com/in/chaitanya1chawla/?originalSubdomain=de" target="_blank">C. Chawla</a>, -->
					<a href="https://www.cs.cmu.edu/~./jeanoh/" target="_blank">J. Oh</a> 
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>Aligning Human and Robot Representations Workshop, Conference on Robot Learning, December 2022</font>
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>In Submission to International Conference on Robotics and Automation, ICRA 2024</font> -->
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://aligning-robot-human-representations.github.io/docs/camready_9.pdf"  target="_blank">Paper</a> /					
					<a href="https://sites.google.com/view/interaction-abstractions" target="_blank">Website</a> /
					<a href="https://github.com/tanmayshankar/CausalSkillLearning" target="_blank">Code</a> /
					<a href="https://www.youtube.com/watch?v=DhgBu8IYUEo" target="_blank">Video</a>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
					Inspired by the success of my work learning representations of robot skills, I'm exploring whether we can apply equivalent machinery to learning temporal abstractions of environment state. In particular, I hope to learn representations of patterns of motion of objects in the environment, or patterns of change of state.
					</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- TRS -->
		<!-- ###################################################################### -->
		
		<tr>
			<td width="25%" align="center">				
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/TRS2.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
					Translating Robot Skills: Learning Unsupervised Skill Correspondences across Humans and Robots
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,
					<a href="https://yixinlin.net/" target="_blank">Y. Lin</a>,
					<a href="https://aravindr93.github.io/" target="_blank">A. Rajeswaran</a>,
					<a href="https://vikashplus.github.io/" target="_blank">V. Kumar</a>,
					<a href="https://www.linkedin.com/in/stuartoanderson" target="_blank">S. Anderson</a>, and
					<a href="https://www.cs.cmu.edu/~./jeanoh/" target="_blank">J. Oh</a>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>International Conference on Machine Learning, July 2022</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://proceedings.mlr.press/v162/shankar22a/shankar22a.pdf"  target="_blank">Paper</a> /
					<a href="https://icml.cc/virtual/2022/spotlight/16262" target="_blank">Talk</a> /
					<a href="https://sites.google.com/view/translatingrobotskills/home" target="_blank">Website</a> /
					<a href="https://github.com/tanmayshankar/CausalSkillLearning" target="_blank">Code</a>					
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
					We developed an unsupervised approach to learn correspondences between skills across humans and various morphologically different robots, taking inspiration from unsupervised machine translation. Our approach is able to learn semantically meaningful  orrespondences between skills across multiple robot-robot and human-robot domain pairs, despite being completely unsupervised.
					</p>
				</div>
			</td>
		</tr> 


		<!-- ###################################################################### -->
		<!-- TDS -->
		<!-- ###################################################################### -->

		
		<tr>
			<td width="25%" align="center">
				<!-- <img src='https://suddhu.github.io/midastouch-tactile/img/midastouch.gif' width="200"> -->
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/Images/TDS.jpg?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
					<!-- Translating Robot Skills: Learning Unsupervised Skill Correspondences across Humans and Robots -->
					<!-- Learning Abstract Representations of Agent Environment Interactions -->
					Translating Dextrous Manipulation Skills across Human and Robot Hands
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,
					<a href="https://www.linkedin.com/in/almutwakel-hassan-147772214/" target="_blank">A. Hassan</a>,
					<a href="https://www.cs.cmu.edu/~./jeanoh/" target="_blank">J. Oh</a>
					
						<div style="height:5px;font-size:1px;">&nbsp;</div>
						<font color=#696969> In Preparation</font>													 
						<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
						[<font color=#009933>Oral: 6% acceptance rate</font>] -->
						<div style="height:5px;font-size:1px;">&nbsp;</div> 

					<!-- <a href="https://aligning-robot-human-representations.github.io/docs/camready_9.pdf"  target="_blank">Paper</a> / -->
					<!-- <a href="https://icml.cc/virtual/2022/spotlight/16262" target="_blank">Talk</a> / -->
					<!-- <a href="https://sites.google.com/view/interaction-abstractions" target="_blank">Website</a> / -->
					<a href="https://github.com/tanmayshankar/CausalSkillLearning" target="_blank">Code</a>					
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
					Inspired by the success of my work translating skills across human and robot arms, I'm exploring whether we can apply equivalent strategies to translating dextrous manipulation skills across human and robot hands.
					<!-- We developed an unsupervised approach to learn correspondences between skills across humans and various morphologically different robots, taking inspiration from unsupervised machine translation. Our approach is able to learn semantically meaningful  orrespondences between skills across multiple robot-robot and human-robot domain pairs, despite being completely unsupervised. -->

					</p>
				</div>
			</td>
		</tr> 
		
		

		<!-- ###################################################################### -->
		<!-- LRS -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/LRS.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>
					Learning Robot Skills with Temporal Variational Inference
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,
					<a href="https://www.cs.cmu.edu/~./abhinavg/" target="_blank">A. Gupta</a>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>International Conference on Machine Learning, July 2020</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="http://proceedings.mlr.press/v119/shankar20b/shankar20b.pdf"  target="_blank">Paper</a> /
					<a href="https://slideslive.com/38927968/learning-robot-skills-with-temporal-variational-inference?ref=recommended" target="_blank">Talk</a> /
					<a href="https://sites.google.com/view/learning-causal-skills" target="_blank">Website</a> /
					<a href="https://github.com/tanmayshankar/CausalSkillLearning" target="_blank">Code</a>					
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					<p><p style = "text-align:justify">
					We presented an unsupervised approach to learn robot skills from demonstrations. 
					We formulated a temporal variational inference, to learn robot skills from demonstrations in an entirely unsupervised manner, while also affording a learnt representation space of skills across a variety of robot and human characters. 
					</p>

				</div>
			</td>
		</tr> 
		
		<!-- ###################################################################### -->
		<!-- DMP -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/DMP.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
					Discovering Motor Programs by Recomposing Demonstrations
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,
					<a href="http://shubhtuls.github.io/" target="_blank">S. Tulsiani</a>,
					<a href="https://www.lerrelpinto.com/" target="_blank">L. Pinto</a>,
					<a href="https://www.cs.cmu.edu/~./abhinavg/" target="_blank">A. Gupta</a>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>International Conference on Learning Representations, April 2020</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://openreview.net/forum?id=rkgHY0NYwr"  target="_blank">Paper</a> /
					<a href="https://iclr.cc/virtual_2020/poster_rkgHY0NYwr.html" target="_blank">Talk</a> /
					<a href="https://sites.google.com/view/discovering-motor-programs/home" target="_blank">Website</a> /
					<a href="https://github.com/tanmayshankar/DiscoverMotorPrograms" target="_blank">Code</a>					
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Learning robot skills from demonstrations using a temporal alignment loss to recompose demonstrations from skills.
				</div>
			</td>
		</tr> 
		
		<!-- ###################################################################### -->
		<!-- LNP -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/Images/LNP.jpg?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
					Learning Neural Parsers with Deterministic Differentiable Imitation Learning
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,
					<a href="https://nrhinehart.github.io/" target="_blank">N. Rhinehart</a>,
					<a href="https://www.linkedin.com/in/katharina-muelling-a6260780/" target="_blank">K. Muelling</a>,
					<a href="https://kriskitani.github.io/" target="_blank">K. Kitani</a>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>Conference on Robot Learning, CoRL, October 2018</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="http://proceedings.mlr.press/v87/shankar18a/shankar18a.pdf"  target="_blank">Paper</a> /
					<a href="https://video.ethz.ch/events/2018/corl/4a7708ac-fcb6-4704-83a6-8345ba722540.html?time=13m13s" target="_blank">Talk</a> /
					<a href="https://github.com/tanmayshankar/ParsingbyImitation" target="_blank">Code</a>
					<!-- <a href="https://youtu.be/L-h8t9-iSFE" target="_blank">Presentation</a> -->
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Learning image parsers by imitating ID3 style decision tree oracles, using differentiable variants of imitation learning.
				</div>
			</td>
		</tr> 

		<!-- ###################################################################### -->
		<!-- RLN -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/Images/RLN.jpg?raw=true' width="300">		                  
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
					Reinforcement Learning via Recurrent Convolutional Neural Networks
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,
					<a href="https://www.iitg.ac.in/engfac/dwivedy/public_html/index.html" target="_blank">S. K. Dwivedy</a>,
					<a href="https://www.iitg.ac.in/eee/pguha.html" target="_blank">P. Guha</a>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>International Conference on Pattern Recognition, ICPR, Cancun 2016</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://arxiv.org/abs/1701.02392"  target="_blank">Paper</a> /
					<!-- <a href="https://sites.google.com/view/discovering-motor-programs/home" target="_blank">Website</a> / -->					
					<a href="https://www.youtube.com/watch?v=gpwA3QNTPOQ" target="_blank">Talk</a> /
					<a href="https://github.com/tanmayshankar/RCNN_MDP" target="_blank">Code</a> 				
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Representing classical computations in Markov Decision Processes within architectures of Recurrent Convolutional Neural Networks. 
				</div>
			</td>
		</tr> 
		</table>

	<br>
	<div style="height:20px;font-size:1px;">&nbsp;</div>

	<hr>
	<br>


	<!-- ###################################################################### -->
	<!-- Projects -->
	<!-- ###################################################################### -->
		
	<h2>Assistive Technology Research</h2>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

		<!-- ###################################################################### -->
		<!-- HyAWET -->
		<!-- ###################################################################### -->

		  <tr>
			<td width="25%" align="center">
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/HyAWET.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
					A Hybrid Assistive Wheelchair Exoskeleton
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,
					<a href="https://www.iitg.ac.in/engfac/dwivedy/public_html/index.html" target="_blank">S. K. Dwivedy</a>,
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>International Convention on Rehabilitation Engineering and Assistive Technology, i-CREATe 2015</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://dl.acm.org/citation.cfm?id=2846727"  target="_blank">Paper</a> /
					<a href="https://www.youtube.com/watch?v=KfomwSxaSbc" target="_blank">Talk</a> 
					<!-- <a href="https://github.com/tanmayshankar/RCNN_MDP" target="_blank">Code</a> 				 -->
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Designing and prototyping a hybrid wheelchair exoskeleton for assisted mobility. 
				</div>
			</td>
		</tr> 
		
		<!-- ###################################################################### -->
		<!-- Invisyble -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/Invisyble.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
						Development of an Assistive Stereo Vision System
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,					
					<a href="https://www.cs.cmu.edu/~abhijatb/" target="_blank">A. Biswas</a>,
					<a href="https://people.csail.mit.edu/venkatar/" target="_blank">S. K. Dwivedy</a>,
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>International Convention on Rehabilitation Engineering and Assistive Technology, i-CREATe 2015</font>
					<!-- <font color=#696969><em>International Conference on Machine Learning</em>, July 2022</font> -->
					<!-- <div style="height:5px;font-size:1px;">&nbsp;</div>
					[<font color=#009933>Oral: 6% acceptance rate</font>] -->
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://dl.acm.org/citation.cfm?id=2846716"  target="_blank">Paper</a> /
					<a href="https://www.youtube.com/watch?v=oQcY104Zt-U&feature=emb_logo" target="_blank">Talk</a> 
					<!-- <a href="https://github.com/tanmayshankar/RCNN_MDP" target="_blank">Code</a> 				 -->
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Designing and prototyping an assistive vision system for blind individuals. 
				</div>
			</td>
		</tr> 
	   </table>

	   
	<!-- ###################################################################### -->
	<!-- Other Projects -->
	<!-- ###################################################################### -->
		
	<h2>Other Projects</h2>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

		<!-- ###################################################################### -->
		<!--HRL-->
		<!-- ###################################################################### -->

		  <tr>
			<td width="25%" align="center">
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/HRL.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
					Hierarchical Reinforcement Learning for Sequencing Behaviors
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://hadisalman.com/" target="_blank">H. Salman</a>,
					<a href="https://www.linkedin.com/in/puneetsinghal90/" target="_blank">P. Singhal</a>,
					<u>T. Shankar</u>,
					et. al. 
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>Course Project, Deep Learning, CMU</font>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://arxiv.org/pdf/1803.01446.pdf"  target="_blank">Paper</a>					
					<a href="https://www.youtube.com/watch?v=-CkI0t_8xUk&ab_channel=HadiSalman" target="_blank">Video 1</a>
					<a href="https://www.youtube.com/watch?v=s5yEO7JqwVY&ab_channel=HadiSalman" target="_blank">Video 2</a>					
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Using hierarchical reinforcement learning to sequence predefined primitives. 
				</div>
			</td>
		</tr> 
		
		<!-- ###################################################################### -->
		<!-- DVPF -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/DVPF.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
					Learn Vector Policy Fields for Continuous Control
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<u>T. Shankar</u>,					
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>Course Project, Deep Reinforcement Learning, CMU</font>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/papers/tanmay_deepvectorpolicyfields.pdf"  target="_blank">Paper</a> /
					<a href="https://www.youtube.com/watch?v=GjJNbsTeJqE&ab_channel=TanmayShankar" target="_blank">Video</a> 
					<!-- <a href="https://github.com/tanmayshankar/RCNN_MDP" target="_blank">Code</a> 				 -->
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Exploring how reinforcement learning networks can be applied to continuous quadrotor control. 
				</div>
			</td>
		</tr> 

		<!-- ###################################################################### -->
		<!-- SISL -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/SISL.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
					Cooperative Vision based Collision Avoidance for Unmanned Aircraft
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://www.linkedin.com/in/eric-mueller-231b755/">E. Muller</a>,
					<u>T. Shankar</u>,			
					<a href="https://mykel.kochenderfer.com/">M. Kochenderfer</a>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>Internship Project, Stanford Intelligent Systems Lab</font>				
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/papers/Cooperative%20Vision-Based%20Collision%20Avoidance%20for%20Unmanned%20Aircraft.pdf"  target="_blank">Paper</a> /
					<a href="https://www.youtube.com/watch?v=xQATigz05-0&feature=emb_title&ab_channel=TanmayShankar" target="_blank">Video</a> 
					<!-- <a href="https://github.com/tanmayshankar/RCNN_MDP" target="_blank">Code</a> 				 -->
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Applying visual SLAM to quadrotors, to enable them with cooperative collision avoidance. 
				</div>
			</td>
		</tr> 

		<!-- ###################################################################### -->
		<!-- IWAMP -->
		<!-- ###################################################################### -->

		<tr>
			<td width="25%" align="center">
				<img src='https://github.com/tanmayshankar/tanmayshankar.github.io/blob/master/data/media/GIFS/IWAMP.gif?raw=true' width="300">
			</td>
			<td valign="center" width="70%">
				<div id="summary">
					<papertitle>				
					Localization for an Interior Wing Assembly Mobile Platform
					</papertitle>                   
					<div style="height:5px;font-size:1px;">&nbsp;</div>					
					<u>T. Shankar</u>,
					<a href="https://www.linkedin.com/in/lu-li-robotics/" target="_blank">L. Li</a>,
					<a href="https://www.cs.cmu.edu/~./choset/" target="_blank">H. Choset</a>,			
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<font color=#696969>Summer Internship Project, CMU, 2014</font>
					<div style="height:5px;font-size:1px;">&nbsp;</div>
					<a href="https://www.youtube.com/watch?v=_sbNZ0pKnAQ&ab_channel=TanmayShankar" target="_blank">Video</a>
					<div style="height:15px;font-size:1px;">&nbsp;</div>
				</div>
				<div id="detail">
				<i>
					Using AR Tag based localization for an interior wing assembly platform. 
				</div>
			</td>
		</tr> 
		

	</table>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
		<p align="right"><font size="2" color=#696969>
	Last updated: Jan 2023
		<p align="right"><font size="2" color=#696969>
<a href="http://www.cs.berkeley.edu/~barron/" target="_blank"><font size="2">Imitation is the highest form of flattery
</a>
</font></p>

<script type="text/javascript">
  var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
  document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
  try {
	var pageTracker = _gat._getTracker("UA-7580334-1");
	pageTracker._trackPageview();
  } catch (err) {}
</script>
</td></tr>
</table>

</body></html>
