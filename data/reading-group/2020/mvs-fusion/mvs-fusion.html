<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Multi-View Stereo by Temporal Nonparametric Fusion</title><style>
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	text-indent: -1.7em;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, YuMincho, "Yu Mincho", "Hiragino Mincho ProN", "Hiragino Mincho Pro", "Songti TC", "Songti SC", "SimSun", "Nanum Myeongjo", NanumMyeongjo, Batang, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC', 'Noto Sans CJK KR'; }

.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC', 'Noto Sans Mono CJK KR'; }

.highlight-default {
}
.highlight-gray {
	color: rgb(155,154,151);
}
.highlight-brown {
	color: rgb(100,71,58);
}
.highlight-orange {
	color: rgb(217,115,13);
}
.highlight-yellow {
	color: rgb(223,171,1);
}
.highlight-teal {
	color: rgb(15,123,108);
}
.highlight-blue {
	color: rgb(11,110,153);
}
.highlight-purple {
	color: rgb(105,64,165);
}
.highlight-pink {
	color: rgb(173,26,114);
}
.highlight-red {
	color: rgb(224,62,62);
}
.highlight-gray_background {
	background: rgb(235,236,237);
}
.highlight-brown_background {
	background: rgb(233,229,227);
}
.highlight-orange_background {
	background: rgb(250,235,221);
}
.highlight-yellow_background {
	background: rgb(251,243,219);
}
.highlight-teal_background {
	background: rgb(221,237,234);
}
.highlight-blue_background {
	background: rgb(221,235,241);
}
.highlight-purple_background {
	background: rgb(234,228,242);
}
.highlight-pink_background {
	background: rgb(244,223,235);
}
.highlight-red_background {
	background: rgb(251,228,228);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(55, 53, 47, 0.6);
	fill: rgba(55, 53, 47, 0.6);
}
.block-color-brown {
	color: rgb(100,71,58);
	fill: rgb(100,71,58);
}
.block-color-orange {
	color: rgb(217,115,13);
	fill: rgb(217,115,13);
}
.block-color-yellow {
	color: rgb(223,171,1);
	fill: rgb(223,171,1);
}
.block-color-teal {
	color: rgb(15,123,108);
	fill: rgb(15,123,108);
}
.block-color-blue {
	color: rgb(11,110,153);
	fill: rgb(11,110,153);
}
.block-color-purple {
	color: rgb(105,64,165);
	fill: rgb(105,64,165);
}
.block-color-pink {
	color: rgb(173,26,114);
	fill: rgb(173,26,114);
}
.block-color-red {
	color: rgb(224,62,62);
	fill: rgb(224,62,62);
}
.block-color-gray_background {
	background: rgb(235,236,237);
}
.block-color-brown_background {
	background: rgb(233,229,227);
}
.block-color-orange_background {
	background: rgb(250,235,221);
}
.block-color-yellow_background {
	background: rgb(251,243,219);
}
.block-color-teal_background {
	background: rgb(221,237,234);
}
.block-color-blue_background {
	background: rgb(221,235,241);
}
.block-color-purple_background {
	background: rgb(234,228,242);
}
.block-color-pink_background {
	background: rgb(244,223,235);
}
.block-color-red_background {
	background: rgb(251,228,228);
}
.select-value-color-default { background-color: rgba(206,205,202,0.5); }
.select-value-color-gray { background-color: rgba(155,154,151, 0.4); }
.select-value-color-brown { background-color: rgba(140,46,0,0.2); }
.select-value-color-orange { background-color: rgba(245,93,0,0.2); }
.select-value-color-yellow { background-color: rgba(233,168,0,0.2); }
.select-value-color-green { background-color: rgba(0,135,107,0.2); }
.select-value-color-blue { background-color: rgba(0,120,223,0.2); }
.select-value-color-purple { background-color: rgba(103,36,222,0.2); }
.select-value-color-pink { background-color: rgba(221,0,129,0.2); }
.select-value-color-red { background-color: rgba(255,0,26,0.2); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="6a77a015-92e1-4efa-807f-a27bc057f89f" class="page sans"><header><div class="page-header-icon undefined"><span class="icon">ðŸ“¹</span></div><h1 class="page-title">Multi-View Stereo by Temporal Nonparametric Fusion</h1></header><div class="page-body"><p id="b071ee81-0257-42e9-8805-8598ecd92fec" class=""><a href="https://aaltoml.github.io/GP-MVS/">https://aaltoml.github.io/GP-MVS/</a></p><p id="c45b0e2a-a2ae-4553-b6bc-2a9da1d22428" class="">Problem: Accurate, real-time dense depth estimation from monocular sequence </p><p id="89b48328-079d-4904-88b2-19df58383370" class="">Idea: Similar views should have similar representations in latent-space</p><p id="36020c83-900a-4e08-8142-aeb8e1619be7" class="">Solution: Soft-constrain bottleneck layer of depth estimation network using Gaussian Procceses (GPs) with appropriate pose-kernel</p><p id="084368ba-0ecd-4c51-86e7-955ed6a3557b" class="">
</p><p id="e5fb84d0-6a30-4f62-8848-a7db19aa04d6" class="">First, we go into the problem of estimating depth for a single view (using multiview constraints in this case)</p><h1 id="ea197c98-125f-4327-a08c-33364e4fd8f8" class="">Network Architecture for Single Frame Depth Estimation</h1><p id="d4dd9b19-bf72-465e-a045-37970435bc43" class="">Exact same architecture as in MVDepthNet: Real-time Multiview Depth Estimation Neural Network</p><p id="b7950ca4-bcf5-4e1f-a0e1-69ccab852123" class="">
</p><p id="e2a8f305-8120-47af-84d1-c92069e5034e" class="">Problem: Leverage multiview geometry in real-time dense depth network 
(assuming known poses of images from odometry source)</p><p id="6ed7cb94-2411-4679-a18e-1d2a17187a25" class="">Idea: Cost volumes can directly encode geometric constraints so that networks don&#x27;t have to</p><p id="7f0b93fc-8463-4dc1-86dc-bc52e1b58e23" class="">Solution: Feed cost volume along with input image into network</p><p id="8b372313-05da-461c-800c-f21ed1fe0191" class="">
</p><p id="91d53b76-bdb6-4f73-9121-ac3f4b2d1680" class="">Methods prior to this work:</p><ul id="c48d96fc-87c9-4d22-9591-c43942d6ad66" class="bulleted-list"><li>Single-view methods do not generalize well due to lack of constraints</li></ul><ul id="770f90d8-3c8d-4fc7-895a-36781ff43e3d" class="bulleted-list"><li>Difficult to parameterize epipolar geometry into network</li></ul><ul id="a1cb6259-8c46-433b-bd5b-3141daf43f47" class="bulleted-list"><li>Cannot easily augment dataset due to multiview constraints</li></ul><h3 id="6f758e2b-8f59-4b42-bd63-1df904eb273f" class="">Cost Volume Construction</h3><ul id="91f02eef-0175-428c-93d2-78a2b1dcda96" class="bulleted-list"><li>Construct cost-volume with respect to a reference frame and auxiliary frames</li></ul><ul id="57885664-89ea-431c-8aa5-64ea5ce9d429" class="bulleted-list"><li>Discretize inverse depth (here using 64 values)</li></ul><figure id="0a440300-b285-4ed6-b14c-15e491089ef0" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled.png"/></a></figure><ul id="a307feb7-4c4b-457c-b909-5a91192546ee" class="bulleted-list"><li>Note: No window size used for cost aggregation (only single pixel used) so that details are preserved</li></ul><p id="349c8527-91e9-4f65-829d-216321f05098" class="">
</p><h3 id="025aeb99-ef6e-4840-b26d-82cee677fd55" class="">Network Architecture</h3><p id="2caf817d-83fe-483f-9eae-0ae3ea4c66d3" class="">Feed RGB reference frame (depth 3) and cost volume (depth 64) into encoder-decoder network with skip connections</p><figure id="8894a46a-4e39-46cc-b553-3dad11113393" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%201.png"><img style="width:2230px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%201.png"/></a></figure><p id="dcf77de6-9d0e-4cb4-b9c2-ae978328a27b" class="">Data augmentation is now straightforward because all inputs and ground-truth are with respect to the same frame</p><p id="e4346666-ef3a-4844-89b7-dbe29b3e2eb6" class="">
</p><h3 id="f8731107-f868-4ec9-9780-9aadc090bf78" class="">Notable Ablation Studies</h3><p id="f6893da1-d18c-4a4b-a90c-2f17a8cbff1c" class="">Example of how adding RGB image to inputs provides finer details as compared to cost-volume alone:</p><figure id="638eb903-9014-454d-95fd-d9d54821815c" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%202.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%202.png"/></a></figure><p id="9b791964-edba-41ca-b1bd-142c259eb92b" class="">
</p><p id="77cc110f-1469-4348-bf2a-47e477bba889" class="">Impact of number of auxiliary frames on accuracy of depth estimation:</p><figure id="e340ff48-8ce9-4f54-9468-4e90749c7486" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%203.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%203.png"/></a></figure><p id="9c2a2eaf-c9f4-4704-b6e7-6f40cb4013f4" class="">
</p><h3 id="6505e302-c466-4dc4-bc09-7935eda4fb6a" class="">Problems</h3><ul id="972d3b19-bc6e-4fbf-99d0-34aded8584e4" class="bulleted-list"><li>While MVDepthNet uses a window of frames, it cannot leverage information beyond this window</li></ul><ul id="5f7dcb3b-0dad-4029-a3eb-2c88e7687e0d" class="bulleted-list"><li>If places revisited, will attempt independent estimation of depth and lack consistency</li></ul><ul id="488132cc-711b-4a98-9842-37f1be03710c" class="bulleted-list"><li>To address this issue, latent representations can enforce temporal consistency between different frames</li></ul><h1 id="af8bc4f0-a4b8-4042-99d2-21b9e4999923" class="">Pose-Kernel Gaussian Process Prior</h1><h3 id="89f771be-c73b-436e-b70b-7363da1aabb7" class="">Gaussian Process Reminder</h3><ul id="b7332b5d-1096-4c98-92cb-caf553b403e9" class="bulleted-list"><li>Nonparametric method often used for regression</li></ul><ul id="657068d3-26a6-4886-8002-3a6798f39074" class="bulleted-list"><li>Observed function values can be used to infer posterior distribution over functions</li></ul><ul id="0e8639e1-24ec-4cfe-bec0-0767350e9e14" class="bulleted-list"><li>Condition on training data to retrieve predictions for test data</li></ul><p id="0737a837-80cc-453e-b522-cd268f16b1c0" class="">
</p><p id="d069cdf4-d454-40a0-8c4a-046d7f645540" class="">Figure generated using <a href="https://distill.pub/2019/visual-exploration-gaussian-processes/">https://distill.pub/2019/visual-exploration-gaussian-processes/</a></p><figure id="8497a035-be43-4a89-a913-b77aad876076" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%204.png"><img style="width:816px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%204.png"/></a></figure><h3 id="603a39af-d156-4111-8936-42041917a316" class="">Pose Kernel</h3><ul id="3fcbfc23-cb1f-45a0-ad2e-2df25999f91b" class="bulleted-list"><li>For kernel to assess similarity between poses, we need some a distance metric between rigid-body poses:</li></ul><figure id="e88d3920-5f0f-4592-92ac-587082820084" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%205.png"><img style="width:480px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%205.png"/></a></figure><ul id="bad6249b-e40b-4493-bccf-697f5f9903f3" class="bulleted-list"><li>Matern kernel used:</li></ul><figure id="e0e2a14c-30d7-4729-9b10-aab2ef0dbbb7" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%206.png"><img style="width:480px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%206.png"/></a></figure><ul id="96092137-a85f-4ee2-a202-70d2956bc1bd" class="bulleted-list"><li>Two learnable parameters here: amplitude <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î³</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">Î³</span></span></span></span></span><span>ï»¿</span></span> and length-scale <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">â„“</mi></mrow><annotation encoding="application/x-tex">\ell</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord">â„“</span></span></span></span></span><span>ï»¿</span></span>.</li></ul><p id="90568ab3-e668-48d5-86bd-648b7cb87109" class="">
</p><h3 id="8c02414c-0e5f-4ecf-8103-b4e2d0f16bc4" class="">Gaussian Process Formulation</h3><ul id="70a1a8c4-8694-4f69-a32d-d8ab31a00a0b" class="bulleted-list"><li>Independent GP priors are placed on all latent space values <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">z_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></span><span>ï»¿</span></span> for <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2...</mn><mo stretchy="false">(</mo><mn>512</mn><mo>Ã—</mo><mn>8</mn><mo>Ã—</mo><mn>10</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">j = 1,2...(512 \times 8 \times 10)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">2</span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mopen">(</span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span><span>ï»¿</span></span></li></ul><figure id="bc3951e9-ce73-4699-9b16-4c38d7acf62c" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%207.png"><img style="width:432px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%207.png"/></a></figure><ul id="9f4d90c9-2209-4bf5-b8fe-5fb69443ef5d" class="bulleted-list"><li>The third learnable GP parameter is <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ïƒ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">Ïƒ</span></span></span></span></span><span>ï»¿</span></span>, which is the noise standard deviation.  </li></ul><p id="8a8b6a0d-d00a-4007-afc9-11544ef3da1f" class="">
</p><p id="d4df7d34-d4f9-421a-bbc4-328dc686b950" class="">Graphical model overview (online method):</p><figure id="bbc902dd-3c9e-472d-bb77-82fc4a790c25" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%208.png"><img style="width:480px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%208.png"/></a></figure><h1 id="d3e97e1e-6471-4619-bfae-8e3ce6afe81b" class="">Estimation</h1><p id="6f80cc02-006a-4d08-8f69-22be0154ebae" class="">Batch method can account for relationships between all poses while online method only has linear complexity</p><h3 id="7a154a11-4502-44aa-b691-84c17474a4df" class="">Batch (Offline)</h3><p id="23d5fe26-ed5c-4866-90f1-e94c8d3280f8" class="">
</p><figure id="1ce557c6-b6a9-4fa3-961f-152c9c3bfc3d" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%209.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%209.png"/></a></figure><ul id="b97ef174-b348-43a5-9ec9-a456f43eb299" class="bulleted-list"><li>Solve GP for all N frames</li></ul><figure id="25929697-5fbb-4d13-ad0e-e8be69d495db" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2010.png"><img style="width:576px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2010.png"/></a></figure><ul id="e7322d99-6ca3-4036-af4e-4bf7accc9b73" class="bulleted-list"><li>Efficient because matrix <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">C</mi></mrow><annotation encoding="application/x-tex">\mathbf{C}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord mathbf">C</span></span></span></span></span></span><span>ï»¿</span></span> only depends on kernel, which only depends on poses, so inverse can be shared across all <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>Ã—</mo><mn>8</mn><mo>Ã—</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">512 \times 8 \times 10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">8</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span></span></span></span></span><span>ï»¿</span></span> subproblems.</li></ul><ul id="9a301f43-109a-4e08-ac55-1cdea47a2bd2" class="bulleted-list"><li>Posterior mean passed through decoder to retrieve the depth map</li></ul><ul id="9718b21d-4be3-475e-abf0-7acf2d8c5419" class="bulleted-list"><li>Matrix inversion limited by number of poses, becomes too expensive after hundreds of frames</li></ul><h3 id="25b28d98-b839-4be7-8fa1-2abc8154117e" class="">Online</h3><figure id="6c4443bb-5a63-48ba-91f4-0c1539154d09" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2011.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2011.png"/></a></figure><ul id="ec3ee30e-bf0c-4f6d-ae27-c8c8305f4046" class="bulleted-list"><li>Relax GP graphical model to be a Markov chain</li></ul><ul id="1458e674-c919-4a02-bba8-c6b70413355b" class="bulleted-list"><li>Reduce the complexity of GP from <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span><span>ï»¿</span></span> to <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span></span><span>ï»¿</span></span>, by reformulating it as a Kalman filtering problem</li></ul><ul id="77ab3a40-5188-4f04-b16d-68fe74db82f4" class="bulleted-list"><li>Convert the problem to a state-space model</li></ul><ul id="a3c37c4d-8dad-4c3e-a790-e5b3288bb723" class="bulleted-list"><li>Latent space encoding conditioned on all image-pose pairs up until current pair</li></ul><h1 id="0c3b8ef6-8abf-4bfb-aaa7-0cb798053a42" class="">Experiments</h1><p id="d3462245-4c61-4390-be2b-c8a1d46223ce" class="">Generally results in improved results</p><figure id="be37d8cd-8b86-43b2-9068-c16ca31404a2" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2012.png"><img style="width:2100px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2012.png"/></a></figure><p id="685dedc2-9f3f-424e-b553-5b93df71f5e8" class="">
</p><p id="66600522-66ff-4a8c-8496-094d3aa67575" class="">Mistakes made early in online method can take more observations to correct since they are propagated forward</p><figure id="24f99ebc-a856-463a-924d-58ba511ae3db" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2013.png"><img style="width:1924px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2013.png"/></a></figure><p id="e021d457-1b8d-491c-a55b-395473b8cfc7" class="">
</p><p id="816c73c9-ac75-4a5a-89c9-f83a5cd65015" class="">Comparison of different kernels on results</p><figure id="8fab9fa6-b75a-4f91-a098-d0d55a232477" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2014.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2014.png"/></a></figure><p id="83a9f267-216c-4c94-92d4-83deef89a412" class="">
</p><p id="34cf40be-1310-430c-bed5-e4c751f5cf6f" class="">This method with 2 frames outperforms other methods with 5 frames</p><figure id="7b933e04-b2ba-4f54-963a-731eb93b6b7c" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2015.png"><img style="width:912px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2015.png"/></a></figure><h1 id="5e1e1745-1e9c-49eb-b3c9-5a687114e5b2" class="">Discussion</h1><h3 id="7bc484f3-c748-406d-8e83-3c4eb500ac20" class="">Online Method Limitations</h3><ul id="58d33fad-334d-4e7a-b3cb-ed62a0818d62" class="bulleted-list"><li>No mention of offline runtime?</li></ul><ul id="aa5de211-e981-4fee-b85f-9ce9cbe4539a" class="bulleted-list"><li>Switching to online method loses many of the benefits proposed by this formulation (online method has an implicit window-size)</li></ul><p id="6e244e36-c594-4143-8831-74857da977c6" class="">
</p><h3 id="9e594396-21c8-4684-b734-ea9a7d94f718" class="">Generalization</h3><ul id="55d178a1-c2ff-4b73-a983-b576f1b05770" class="bulleted-list"><li>While very efficient and provides improvement in indoor scenes, generalization should be discussed further</li></ul><p id="49e85ff3-9802-4c7c-9345-2d703ef6d99b" class="">
</p><ul id="e380397b-7993-4ea3-9d8b-c996608497d2" class="bulleted-list"><li>Does this method help for some motions more than others? (Rotation vs. translation)<ul id="3e2d36e6-e11f-45d0-8980-85d590c7da29" class="bulleted-list"><li>MVDepthNet cost volume will lack information for pure rotation?</li></ul><ul id="221503bc-5fe9-4a97-9807-64e810a7a626" class="bulleted-list"><li>This method can  propagate future information back into past, while MVDepthNet cannot</li></ul></li></ul><p id="82ba91d9-56af-4804-90af-64dbdc08d385" class="">
</p><ul id="1b3f3520-7155-43bb-b082-313d26c55dca" class="bulleted-list"><li>Gaussian Process lacks information about size of scene<ul id="77d67460-533b-45c8-a94f-1b32e5e5a309" class="bulleted-list"><li>Length-scale parameter learned, but may not generalize to larger scenes</li></ul><ul id="0269bfd8-f1c4-4e78-b6ff-cc3283f72cf9" class="bulleted-list"><li>Influence of poses within 1m very different depending on indoor vs. outdoor scene</li></ul><ul id="ae00402c-47ea-4b7e-a381-bbad91407a99" class="bulleted-list"><li>Kernels could be learned, but pose information alone may not be sufficient</li></ul></li></ul><p id="73fd327c-f695-4ac1-aa83-6b1de2941dcb" class="">
</p><ul id="24970e8a-c50b-4ef6-b9ef-7e57ab97a129" class="bulleted-list"><li>Computation may be severely limited if more information was included, from Section 3.3:<ul id="73180714-b7ce-4d2a-97c9-fc5a070ce95b" class="bulleted-list"><li>&quot;Because the likelihood is Gaussian and all the GPs share the same poses at which the covariance function is evaluated,we may solve all the 512Ã—8Ã—10GP regression problems with one matrix inversion. This is due to the posterior co-variance only being a function of the input poses, not values of learnt representations of images (i.e.,y does not appear in the posterior variance terms in Eq. 5)&quot;</li></ul></li></ul><h3 id="1c7208d3-759c-4976-af5b-d9e8eb01f15e" class="">Graphical Models and Refinement of Latent Space</h3><p id="20649cef-1ca5-46c3-9169-de395ca70fab" class="">Interesting to think about ways of utilizing latent space in sequential estimation</p><ul id="f553dd28-6920-4671-b267-d537131d6ae6" class="bulleted-list"><li>CodeSLAM propagates observed errors to refine latent space
(geometric and photometric errors should be minimized)</li></ul><figure id="3423980b-9a68-4833-a562-ec80c3f55368" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2016.png"><img style="width:528px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2016.png"/></a></figure><ul id="98aa9eec-f30b-41c1-9161-e94b3f2cfc0c" class="bulleted-list"><li>This work implicitly constrains the latent space according to hand-crafted kernel
(similar poses should have similar latent spaces)</li></ul><p id="1262913d-a77b-49aa-b93d-3a1cdb74eadc" class="">
</p><h3 id="c6af69d0-811f-4e0a-b40e-30b5b7c4c61e" class="">Uncertainty Estimation</h3><ul id="93bc0e9e-e1b3-4b25-9e1d-136912b8069c" class="bulleted-list"><li>Uncertainty estimation especially useful for the downstream tasks we are interested in</li></ul><p id="ce987cea-4a9a-48c6-be99-509b9fe73089" class="">
</p><ul id="c561508d-7909-4a1f-82f2-05bcf09a38c8" class="bulleted-list"><li>Is the uncertainty estimation from GPs useful for certain tasks?<ul id="1368e960-4bb7-44a5-b871-dab41c4e6407" class="bulleted-list"><li>Useful for determining if similar data was previously seen</li></ul><ul id="1ba8b0df-05f8-4db4-b027-52b0a6d4b98b" class="bulleted-list"><li>Maybe not possible to propagate due to skip connections?</li></ul></li></ul><p id="0906c61b-23b2-41b7-88f1-a610f1d166af" class="">
</p><ul id="0fc23667-dbe2-4f81-bd45-e36046e5f2f2" class="bulleted-list"><li>Are there benefits to using GP uncertainty instead of having networks directly predict the uncertainty as an output? (<a href="https://arxiv.org/pdf/1703.04977.pdf">https://arxiv.org/pdf/1703.04977.pdf</a>)<ul id="dcd68000-097d-438a-b385-31f1a70e9397" class="bulleted-list"><li>To model uncertainty, some networks commonly use loss such as
</li></ul></li></ul><figure id="2eec9bc2-1790-482f-884a-9312263e6943" class="image"><a href="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2017.png"><img style="width:384px" src="Multi-View%20Stereo%20by%20Temporal%20Nonparametric%20Fusion%200a440300b2854ed6b14c15e491089ef0/Untitled%2017.png"/></a></figure><ul id="25c7f4cf-a4f9-4ff9-a926-a236edeb846e" class="bulleted-list"><li>GPs can be viewed as infinitely wide neural network layer with i.i.d. prior on weights</li></ul><ul id="0ad87529-e67f-491f-8c8f-48f0c3261f25" class="bulleted-list"><li>GPs give more flexible models, while Bayesian NNs give more scalable models</li></ul><ul id="5370fbde-f94e-42b9-bbc9-371012b42df9" class="bulleted-list"><li>With enough data, is it sufficient to use neural networks?</li></ul><p id="d3b951ef-eca6-4e2a-9d52-a5b4865b4858" class="">
</p><ul id="fdfa07e9-0bfb-42ed-9eb5-00ec93b3c671" class="bulleted-list"><li>Examples of neural network uncertainty usage in dense SLAM<ul id="7930dbc0-03b9-43a4-86ef-720a14d64a03" class="bulleted-list"><li>D3VO: <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_D3VO_Deep_Depth_Deep_Pose_and_Deep_Uncertainty_for_Monocular_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_D3VO_Deep_Depth_Deep_Pose_and_Deep_Uncertainty_for_Monocular_CVPR_2020_paper.pdf</a></li></ul><ul id="16a4e065-8715-4f91-b4ff-b642afe03af7" class="bulleted-list"><li>CodeSLAM: <a href="https://arxiv.org/pdf/1804.00874.pdf">https://arxiv.org/pdf/1804.00874.pdf</a></li></ul></li></ul></div></article></body></html>